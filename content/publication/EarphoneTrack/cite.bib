@inproceedings{10.1145/3384419.3430730,
author = {Cao, Gaoshuai and Yuan, Kuang and Xiong, Jie and Yang, Panlong and Yan, Yubo and Zhou, Hao and Li, Xiang-Yang},
title = {EarphoneTrack: Involving Earphones into the Ecosystem of Acoustic Motion Tracking},
year = {2020},
isbn = {9781450375900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384419.3430730},
doi = {10.1145/3384419.3430730},
abstract = {Acoustic motion tracking is an exciting new research area with promising progress
in the last few years. Due to the inherent low propagation speed in the air, acoustic
signals have the unique advantage of fine sensing granularity compared to RF signals.
Speakers and microphones nowadays are pervasively available in devices surrounding
us, such as smartphones and voice-controlled smart speakers. Though promising, one
fundamental issue hindering the adoption of acoustic-based motion tracking is that
the positions of microphones and speakers inside a device are fixed, which greatly
limits the flexibility of acoustic motion tracking. In this work, we propose a new
modality of acoustic motion tracking using earphones. Earphone-based tracking mitigates
the constraints associated with traditional smartphone-based tracking. With novel
designs and comprehensive experiments, we show earphone-based motion tracking can
achieve a great flexibility and a high accuracy at the same time. We believe this
is an important step towards "earable" sensing.},
booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
pages = {95â€“108},
numpages = {14},
keywords = {earphone-based acoustic sensing, earable sensing, motion tracking},
location = {Virtual Event, Japan},
series = {SenSys '20}
}